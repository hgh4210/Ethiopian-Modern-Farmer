import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import joblib # For saving and loading the model
import os

# --- Page Setup and Title ---
st.set_page_config(page_title="á‹¨á‹ˆá‰°á‰µ áŒ¥áˆ«á‰µ á‰µáŠ•á‰ á‹«", layout="wide")
st.title("ğŸ¥› á‹¨á‹ˆá‰°á‰µ áŒ¥áˆ«á‰µ áŠ¥áŠ“ á‹¨áˆ˜á‰ áˆ‹áˆ¸á‰µ á‰µáŠ•á‰ á‹«")
st.markdown("á‹¨á‹ˆá‰°á‰µá‹áŠ• áˆ˜áˆ¨áŒƒ á‰ áˆ›áˆµáŒˆá‰£á‰µ á‹¨áŒ¥áˆ«á‰µ á‹°áˆ¨áŒƒá‹áŠ• á‹­á‰°áŠ•á‰¥á‹©á¢")

# --- Data Loading and Preparation ---
DATA_FILE_PATH = 'milknew.csv' # Place this file in the same directory

@st.cache_data # Cache data to prevent reloading
def load_and_preprocess_data(file_path):
    try:
        df = pd.read_csv(file_path)
        # Convert 'Grade'
        df['Grade'] = df['Grade'].map({'high': 2, 'medium': 1, 'low': 0})
        # Correcting the space in the 'Fat ' column name
        if 'Fat ' in df.columns:
            df.rename(columns={'Fat ': 'Fat'}, inplace=True)
        # Check for missing values (good practice, though not present in this dataset)
        if df.isnull().sum().any():
            st.warning("á‰ á‹³á‰³á‹ á‹áˆµáŒ¥ á‹¨áŒá‹°áˆ‰ áŠ¥áˆ´á‰¶á‰½ áŠ áˆ‰á£ á‹­áˆ…áˆ á‹¨á‰µáŠ•á‰ á‹«á‹áŠ• á‰µáŠ­áŠ­áˆˆáŠ›áŠá‰µ áˆŠá‰€áŠ•áˆµ á‹­á‰½áˆ‹áˆá¢")
            # df = df.dropna() # or other imputation method
        return df
    except FileNotFoundError:
        st.error(f"áˆµáˆ…á‰°á‰µá¦ á‹¨á‹³á‰³ á‹á‹­áˆ '{file_path}' áŠ áˆá‰°áŒˆáŠ˜áˆá¢ áŠ¥á‰£áŠ­á‹ á‹á‹­áˆ‰ áˆ˜áŠ–áˆ©áŠ• á‹«áˆ¨áŒ‹áŒáŒ¡á¢")
        return None
    except Exception as e:
        st.error(f"á‹³á‰³á‹áŠ• á‰ áˆ˜áŒ«áŠ• áˆ‹á‹­ áˆ³áˆˆ áˆµáˆ…á‰°á‰µ á‰°áŠ¨áˆµá‰·áˆá¦ {e}")
        return None

df_milk = load_and_preprocess_data(DATA_FILE_PATH)

# --- Model Training or Loading ---
MODEL_FILE_PATH = 'milk_quality_rf_model.joblib'
SCALER_FILE_PATH = 'milk_quality_scaler.joblib'

@st.cache_resource # Cache model and scaler (load only once)
def train_or_load_model_and_scaler(data_frame):
    if data_frame is None:
        return None, None

    X = data_frame.iloc[:, :-1]
    y = data_frame.iloc[:, -1]

    # Load model and scaler if they already exist
    if os.path.exists(MODEL_FILE_PATH) and os.path.exists(SCALER_FILE_PATH):
        try:
            model = joblib.load(MODEL_FILE_PATH)
            scaler = joblib.load(SCALER_FILE_PATH)
            # Ensure scaler is fitted (to transform new data)
            # It might be necessary to check the original X_train data size
            # X_train_temp, _, _, _ = train_test_split(X, y, test_size=0.2, random_state=42)
            # scaler.fit(X_train_temp) # or use the pre-fitted one
            st.sidebar.success("á‹¨áˆ°áˆˆáŒ áŠ áˆá‹´áˆ áŠ¥áŠ“ áˆµáŠ¬áˆˆáˆ­ á‰°áŒ­áŠ—áˆá¢")
            return model, scaler
        except Exception as e:
            st.sidebar.warning(f"á‹¨á‰°á‰€áˆ˜áŒ á‹áŠ• áˆá‹´áˆ/áˆµáŠ¬áˆˆáˆ­ áˆ˜áŒ«áŠ• áŠ áˆá‰°á‰»áˆˆáˆ ({e})á¢ áŠ á‹²áˆµ á‰ áˆ›áˆ°áˆáŒ áŠ• áˆ‹á‹­...")

    # Prepare data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # random_state for reproducible results

    # Scaler
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Random Forest model (high accuracy)
    # depth_vec = np.arange(1, 20, 1) # For finding best depth
    # For simplicity, we use a common good depth or the one found in notebook (e.g., 10-15)
    best_depth = 12 # Good depth found from notebook or experience
    model = RandomForestClassifier(max_depth=best_depth, random_state=0)
    model.fit(X_train_scaled, y_train)

    # Evaluate model accuracy
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    st.sidebar.metric(label="á‹¨áˆ™áŠ¨áˆ« áˆá‹´áˆ á‰µáŠ­áŠ­áˆˆáŠ›áŠá‰µ", value=f"{accuracy*100:.2f}%")

    # Save the model and scaler
    try:
        joblib.dump(model, MODEL_FILE_PATH)
        joblib.dump(scaler, SCALER_FILE_PATH)
        st.sidebar.success("áˆá‹´áˆ áŠ¥áŠ“ áˆµáŠ¬áˆˆáˆ­ áˆ°áˆáŒ¥áŠá‹ á‰°á‰€áˆáŒ á‹‹áˆá¢")
    except Exception as e:
        st.sidebar.error(f"áˆá‹´áˆ‰áŠ•/áˆµáŠ¬áˆˆáˆ©áŠ• áˆ›áˆµá‰€áˆ˜áŒ¥ áŠ áˆá‰°á‰»áˆˆáˆá¦ {e}")

    return model, scaler

if df_milk is not None:
    model, scaler = train_or_load_model_and_scaler(df_milk)
else:
    model, scaler = None, None
    st.stop() # Stop the app if data is not available

# --- User Inputs ---
st.sidebar.header("á‹¨á‹ˆá‰°á‰µ áˆ˜áˆ¨áŒƒ á‹«áˆµáŒˆá‰¡")

def user_input_features(data_frame):
    # Get approximate ranges from the data
    ph_min, ph_max = float(data_frame['pH'].min()), float(data_frame['pH'].max())
    temp_min, temp_max = int(data_frame['Temprature'].min()), int(data_frame['Temprature'].max())
    # Taste, Odor, Fat, Turbidity are 0 or 1
    # Colour min, max
    colour_min, colour_max = int(data_frame['Colour'].min()), int(data_frame['Colour'].max())

    ph = st.sidebar.slider('á’áŠ¤á‰½ (pH)', ph_min, ph_max, float(data_frame['pH'].mean()), 0.1)
    temprature = st.sidebar.slider('á‹¨áˆ™á‰€á‰µ áˆ˜áŒ áŠ• (Â°C)', temp_min, temp_max, int(data_frame['Temprature'].mean()))
    taste = st.sidebar.selectbox('áŒ£á‹•áˆ (0=áŠ­á‰, 1=áŒ¥áˆ©)', (0, 1))
    odor = st.sidebar.selectbox('áˆ½á‰³ (0=á‹¨áˆˆáˆ, 1=áŠ áˆˆ)', (0, 1))
    fat = st.sidebar.selectbox('á‹¨áˆµá‰¥ áˆ˜áŒ áŠ• (0=á‹á‰…á‰°áŠ›, 1=áŠ¨áá‰°áŠ›)', (0, 1)) # 'Fat ' was the original column name
    turbidity = st.sidebar.selectbox('á‹°á‰¥á‹›á‹áŠá‰µ (0=á‹¨áˆˆáˆ, 1=áŠ áˆˆ)', (0, 1))
    colour = st.sidebar.slider('á‰€áˆˆáˆ (áŠ¥áˆ´á‰µ)', colour_min, colour_max, int(data_frame['Colour'].mean()))

    data = {
        'pH': ph,
        'Temprature': temprature,
        'Taste': taste,
        'Odor': odor,
        'Fat': fat, # Ensure this matches the renamed column
        'Turbidity': turbidity,
        'Colour': colour
    }
    features = pd.DataFrame(data, index=[0])
    return features

input_df = user_input_features(df_milk)

# --- Displaying Input Data ---
st.subheader('áŠ¥áˆ­áˆµá‹ á‹«áˆµáŒˆá‰¡á‰µ á‹¨á‹ˆá‰°á‰µ áˆ˜áˆ¨áŒƒá¦')
st.write(input_df)

# --- Prediction and Result Display ---
if model and scaler:
    # Scale the input data
    input_df_scaled = scaler.transform(input_df)

    # Prediction
    prediction = model.predict(input_df_scaled)
    prediction_proba = model.predict_proba(input_df_scaled)

    st.subheader('á‹¨á‰µáŠ•á‰ á‹« á‹áŒ¤á‰µá¦')
    grade_map_amharic = {2: "áŠ¨áá‰°áŠ› áŒ¥áˆ«á‰µ (High)", 1: "áˆ˜áŠ«áŠ¨áˆˆáŠ› áŒ¥áˆ«á‰µ (Medium)", 0: "á‹á‰…á‰°áŠ› áŒ¥áˆ«á‰µ (Low)"}
    predicted_grade_amharic = grade_map_amharic.get(prediction[0], "á‹«áˆá‰³á‹ˆá‰€")

    st.success(f"á‹¨á‰°áŒˆáˆ˜á‰°á‹ á‹¨á‹ˆá‰°á‰µ áŒ¥áˆ«á‰µ á‹°áˆ¨áŒƒá¦ **{predicted_grade_amharic}**")

    if prediction[0] == 2: # High
        st.balloons()
    elif prediction[0] == 0: # Low
        st.warning("á‹­áˆ… á‹ˆá‰°á‰µ á‹á‰…á‰°áŠ› áŒ¥áˆ«á‰µ á‹«áˆˆá‹ á‹ˆá‹­áˆ á‹¨áˆ˜á‰ áˆ‹áˆ¸á‰µ áˆµáŒ‹á‰µ áˆŠáŠ–áˆ¨á‹ á‹­á‰½áˆ‹áˆá¢")

    st.subheader('á‹¨áˆ˜á‰°áˆ›áˆ˜áŠ• á‹°áˆ¨áŒƒ (Confidence) áˆˆáŠ¥á‹«áŠ•á‹³áŠ•á‹± á‹¨áŒ¥áˆ«á‰µ áˆ˜á‹°á‰¥á¦')
    proba_df = pd.DataFrame({
        "á‹¨áŒ¥áˆ«á‰µ áˆ˜á‹°á‰¥": [grade_map_amharic[0], grade_map_amharic[1], grade_map_amharic[2]],
        "á‹¨áˆ˜á‰°áˆ›áˆ˜áŠ• á‹•á‹µáˆ": [f"{p*100:.2f}%" for p in prediction_proba[0]]
    })
    st.table(proba_df)

else:
    st.error("áˆá‹´áˆ‰ á‹ˆá‹­áˆ áˆµáŠ¬áˆˆáˆ© áŠ áˆá‰°áŒ«áŠáˆá¢ áŠ¥á‰£áŠ­á‹ áŠ¥áŠ•á‹°áŒˆáŠ“ á‹­áˆáŠ­áˆ© á‹ˆá‹­áˆ á‹¨á‹³á‰³ á‹á‹­áˆ‰áŠ• á‹«áˆ¨áŒ‹áŒáŒ¡á¢")

# (Optional) Display general data overview
st.markdown("---")
if st.checkbox('á‹¨áˆ˜áŒ€áˆ˜áˆªá‹«á‹áŠ• á‹¨á‹ˆá‰°á‰µ á‹³á‰³ áŠ“áˆ™áŠ“ á‹­áˆ˜áˆáŠ¨á‰± (á‰ áŠ¥áŠ•áŒáˆŠá‹áŠ›)'):
    st.subheader('á‹¨á‹ˆá‰°á‰µ á‹³á‰³ áŠ“áˆ™áŠ“ (áŠ¨ milknew.csv):')
    st.write(df_milk.head())
    # Display meaning of 'Grade'
    st.caption("á‹¨'Grade' á‰µáˆ­áŒ‰áˆ á‰ á‹³á‰³á‹ á‹áˆµáŒ¥á¦ 2=High (áŠ¨áá‰°áŠ›), 1=Medium (áˆ˜áŠ«áŠ¨áˆˆáŠ›), 0=Low (á‹á‰…á‰°áŠ›)")

